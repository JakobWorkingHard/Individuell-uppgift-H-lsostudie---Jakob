{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd77d24",
   "metadata": {},
   "source": [
    "******Table of contents******\n",
    "\n",
    "This notebook is split into two parts: Part 1 and Part 2. You can run the part 1 and part 2 separetely from each other's. This report aims to give some insights of the data from \"health_study\", which contains the variables age, sex, smoker(No/Yes), disease(0/1), cholesterol, systolic bloodpressure, height, weight, and 800 datapoints (anonymous individuals). \n",
    "\n",
    "**Part 1:**\n",
    "1. Calculate the mean, median, minimum (min), and maximum (max) for the following variables: age, weight, height, systolic_bp, and cholesterol.\n",
    "2. Create at least 3 different graphs (e.g., a histogram of blood pressure, a box plot of weight per gender, a bar chart of the proportion of smokers).\n",
    "3. \n",
    "- Calculate the proportion of individuals in the dataset who have the disease.\n",
    "- Use NumPy to simulate 1000 random individuals with the same probability of having the disease.\n",
    "-  Compare the simulated proportion with the actual proportion found in the dataset.\n",
    "4. Calculate the confidence interval using two different methods (e.g., normal approximation and bootstrap) and compare the results. (Advanced Requirement - VG)\n",
    "5. Test the hypothesis: \"Smokers have a higher mean blood pressure than non-smokers.\"\n",
    "6. Conduct a simulation to examine the reliability of your hypothesis test (e.g., how often the test finds a difference when one truly exists $\\rightarrow$ statistical power). (Advanced Requirement - VG)\n",
    "\n",
    "**Part 2: Advanced Analysis and Pipeline Development**\n",
    "\n",
    "Code Structure and Modularity\n",
    "1. Structure the Code: Organize the overall code base logically.\n",
    "2. Modularization: Move relevant code segments from Part 1 into reusable functions and modules.\n",
    "3. Object-Oriented Programming (OOP): \n",
    "- Create at least one class (e.g., HealthAnalyzer) capable of performing a specific part of the analysis (e.g., calculating summary statistics or plotting graphs).\n",
    "- Build a more developed class that can handle multiple different analyses or visualizations (Advanced Requirement - VG).\n",
    "4. Applied Linear Algebra: Use NumPy or scikit-learn for an analysis that relies on matrices/vectors. Utilize a more advanced method (e.g., multiple regression) or include an additional visualization that clearly explains the results (Advanced Requirement - VG).\n",
    "5. Deeper Insight: Add at least one new analysis or graph that provides a deeper understanding (e.g., the relationship between blood pressure and age, or disease prevalence per gender).\n",
    "6. Write docstrings for your functions and classes.\n",
    "7. Use Markdown blocks to explain what you are doing and why.\n",
    "8. Justify your choice of methods within the Markdown blocks and refer to documentation or other sources (Advanced Requirement - VG).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596ceb5",
   "metadata": {},
   "source": [
    "**Part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67bcd5",
   "metadata": {},
   "source": [
    "**Import and clean data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d70e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "import math\n",
    "from src.moduls import * \n",
    "from src.io_utils import *\n",
    "from src.viz import *\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = clean_data(read_data(\"dataset/health_study_dataset.csv\"))\n",
    "df.sample(10)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a7a8e",
   "metadata": {},
   "source": [
    "**1. Max, min, mean, median of:**\n",
    "- Age\n",
    "- Weight\n",
    "- Height\n",
    "- Systolic_bp\n",
    "- Cholesterol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a715c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "showing_standard_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45ffb4",
   "metadata": {},
   "source": [
    "**2. a) Boxplot of weight grouped by sexes**\n",
    "\n",
    "**Conclusion: Men are heavier than women when comparing:**\n",
    "- Mean\n",
    "- Q1 to Q3, where men's interval is higher than women's\n",
    "- The normalised interval, where the entire interval (excluding outliers) are higher than women\n",
    "- Funny sidenote, even when looking at the outliers, the low-weight outliers-men are within the women's normal range, and vice versa for the heavier outliers of women, within the normal range of men.\n",
    "\n",
    "**Motivation of using boxplot for this scenario:** My main motivation is best described from this quoto: *\"Boxplots are built to provide high-level information at a glance, offering general information about a group of data’s symmetry, skew, variance, and outliers. It is easy to see where the main bulk of the data is, and make that comparison between different groups.\"*\n",
    "\n",
    "They do however have some limitations, but for this purpose in this specific boxplot, we won't get any of these.\n",
    "\n",
    "- Källa: https://www.atlassian.com/data/charts/box-plot-complete-guide\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gender_weight_difference(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec431fd",
   "metadata": {},
   "source": [
    "**2. b) Comparing the blood pressure between disease and non-disease people, grouped by sex and smokers**\n",
    "\n",
    "**Conclusion by only comparing the bars**\n",
    "- Sick people have, as a group, a higher average bloodpressure than non-sick people in all groups except males who smokes\n",
    "\n",
    "**Motivation of using barplot:** My choice of barplot is best described with this quote: *\"Why Use Bar Plots?\n",
    "Bar plots are significant because they provide a clear and intuitive way to visualize categorical data. They allow viewers to quickly grasp differences in size or quantity among categories, making them ideal for presenting survey results, sales data, or any discrete variable comparisons.\"*\n",
    "\n",
    "And in this case we are comparing different groups (sex and smokers) with the numerical value of systolic bp.\n",
    "\n",
    "https://www.geeksforgeeks.org/pandas/bar-plot-in-matplotlib/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_of_healthy_vs_diseased = healthy_vs_diseased_info(df)\n",
    "plot_disease_vs_healthy(tuple_of_healthy_vs_diseased[1], tuple_of_healthy_vs_diseased[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabe232",
   "metadata": {},
   "source": [
    "**2. c) Comparing age with cholesterol between disease and non-disease people**\n",
    "\n",
    "**Conclusion**\n",
    "- Too few datapoints from the disease-group to actually tell anything but, but looking at age and cholesterol in it's total it seems like cholesterol raises with age.\n",
    "\n",
    "**Motivation of using scatter-plot:** Scatter plot is generally the best choice when we have two numerical variables that we want to check against each other's to see if there could be any correlation ( https://online.visual-paradigm.com/knowledge/data-visualization/what-is-scatter-diagram/ ). However, to make things even clearer we can also use linear regression, to make a linear function within our graph that tries to be as close to as many datapoints are possible ( https://www.britannica.com/topic/linear-regression ) which can then be used to make predictions. However, it must be noted that linear regression does not check for any statistical significance, and this is a great example of that, that despite us having only 47 datapoints, it draws the linear function anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ef908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_of_healthy_vs_diseased_mask = healthy_vs_diseased(df)\n",
    "plot_comparing_age_with_cholesterol(tuple_of_healthy_vs_diseased_mask[0], tuple_of_healthy_vs_diseased_mask[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7852cf1",
   "metadata": {},
   "source": [
    "**3. Disease-frequency vs creating a random dataset with the actual disease frequency**\n",
    "\n",
    "- Blue: Actual disease-frequency\n",
    "- Yellow: Randomised dataset with the actual frequency in mind\n",
    "- Red: Difference between the actual and the random frequency\n",
    "\n",
    "**Motivation of using bar-plot:** Since comparing two categories, one actual and one fictional frequency, we are comparing two groups numerical difference, and that is best plotted with a barplot. See previous motivation from 2 b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2219f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency = actual_frequency_vs_random_generated_frequency(df)\n",
    "plot_actual_frequency_vs_random_generated_frequency(df_frequency)\n",
    "df_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072154b",
   "metadata": {},
   "source": [
    "**4. CI normal approximation and bootstrap of systolic bp**\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Both bootstrap and normal approximation shows a very similar result, indicating that normal approximation is in this scenario a robust way of calculating a confidence interval. If the results from the two methods were to differ significantly, it would indicate that the Central Limit Theorem (CLT) is not sufficiently satisfied. Either because our n is too small for the sampling distribution of the mean to be approximated as normal, or because the underlying data distribution is highly non-normal (skewed, heavy-tailed), which the bootstrap method is better equipped to handle.\n",
    "\n",
    "https://www.youtube.com/watch?v=xjYEYBvPaSc&t=2572s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b925f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_analysis = CI_and_bootstrap(df[\"systolic_bp\"], confidence=0.95)\n",
    "\n",
    "results = ci_analysis.compare_methods(B=10000)\n",
    "\n",
    "results\n",
    "\n",
    "plot_ci_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4addbbf",
   "metadata": {},
   "source": [
    "**Bonus hypothesis-test**: Bloodpressure in healthy vs disease group differs\n",
    "\n",
    "- *H0: Bloodpressure in healthy and disease-group is the same*\n",
    "- *H1: Bloodpressure in healthy and disease-group differs*\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The test didn't find a p-value < 0.05, but rather a p-value > 0.3 which is way too high for a significant result. Therefore, we cannot reject the null hypothesis.\n",
    "\n",
    "**Motivation of why using a two tail t-test:** We want to find out if there are any markers that could give an indication of a person being sick, and we don't know how the disease affects systolic bp, so therefore we want to first figure out if there is a difference between the disease group and the non disease group at all, and so we use a two tail test t-test to see if the mean of the disease systolic bp is actually, in a statistical significant way, different from that of the healthy group. \n",
    "\n",
    "https://statisticsbyjim.com/hypothesis-testing/one-tailed-two-tailed-hypothesis-tests/ <-- Theory\n",
    "https://www.youtube.com/watch?v=jfBuCNus-HY <-- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease, healthy = healthy_vs_diseased(df)\n",
    "\n",
    "stats.ttest_ind(a=disease[\"systolic_bp\"],\n",
    "                b=healthy[\"systolic_bp\"],\n",
    "                equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967260ce",
   "metadata": {},
   "source": [
    "**Bonus power-test: Checking for how often our t-test will actually find a difference if there is one (type 2 error)**\n",
    "\n",
    "- Power: 0.177 with difference between sick and healthy set to: 2mmHg\n",
    "- Power: 0.705 with difference between sick and healthy set to: 5mmHg\n",
    "- Power: 0.849 with difference between sick and healthy set to: 6mmHg\n",
    "- Difference set to greater than 6 mmHg generates a power well above 0.85, which is considered a valid number.\n",
    "\n",
    "**Conclusion:** Our previous t-test will not find small differences between the groups (differences between the group of < 6 mmHg), since our datapoints are too few. However, greater differences between the groups will still be found (> 6 mmHg). If we want our t-test to be more refined for finding even small differences, we need to increase our n greatly.\n",
    "\n",
    "**Motivation of using power-test:** Since we want to find out what kind of differences our previous t-test will actually be able to find, considering our n, we use a power-test so see how many times our t-test finds significant differences, considering the scenario that there actually is a real difference. This is usually a method you want to use before collecting your sample, since it gives an indication of how large our n needs to be. But since we already have our sample, and our t-test couldn't find a significant difference, we want to see for ourselves how refined our t-test actually was.\n",
    "\n",
    "https://www.youtube.com/watch?v=fmEk4L9IlA8&t=1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(disease.describe())     #<-- To check if the correct n and std have been plugged in\n",
    "# print(healthy.describe())     #<-- To check if the correct n and std have been plugged in\n",
    "\n",
    "print(checking_power_of_that_t_test(47,753,13.172255, 12.771772, 2, 0.05, 1000, \"two-sided\"))\n",
    "print(checking_power_of_that_t_test(47,753,13.172255, 12.771772, 5, 0.05, 1000, \"two-sided\"))\n",
    "print(checking_power_of_that_t_test(47,753,13.172255, 12.771772, 6, 0.05, 1000, \"two-sided\"))\n",
    "print(checking_power_of_that_t_test(47,753,13.172255, 12.771772, 8, 0.05, 1000, \"two-sided\"))\n",
    "print(checking_power_of_that_t_test(47,753,13.172255, 12.771772, 10, 0.05, 1000, \"two-sided\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983b68d",
   "metadata": {},
   "source": [
    "**5. Trying hypothesis smokers have higher systolic bp than non smokers**\n",
    "\n",
    "- H0: Smokers does not have higher systolic bp than non smokers\n",
    "- H1: Smokers do have higher systolic bp than non smokers\n",
    "\n",
    "**Conclusion:** p-value = 0.326, which means that we cannot reject our null hypothesis, and cannot conclude that smokers do have higher systolic bp than non smokers. \n",
    "\n",
    "**Motivation:** Here, we only want to check if smokers have a higher systolic bp than non smokers, and we therefore only use a one tail t-test, specifically checking the right side of our normal approximation, while also assuming that the two groups variance can differ from one another (welch test). \n",
    "Links: https://statisticsbyjim.com/hypothesis-testing/one-tailed-two-tailed-hypothesis-tests/ <-- Theory\n",
    "https://www.youtube.com/watch?v=jfBuCNus-HY <-- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smokers, non_smokers = looking_for_them_smokers(df)\n",
    "\n",
    "\n",
    "result = stats.ttest_ind(a=smokers[\"systolic_bp\"],\n",
    "                        b=non_smokers[\"systolic_bp\"],\n",
    "                        equal_var=False,\n",
    "                        alternative=\"greater\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87c686",
   "metadata": {},
   "source": [
    "**6. Power-testing the previous t-test**\n",
    "\n",
    "**Conclusion:** Here, in comparison to our previous power test, we can see that we have a sufficient power of finding a the difference that smokers have a higher systolic bp of greater than 3 mmHg. If the difference is less than 3 mmHg, our test wont have sufficient power, and we might miss the difference. \n",
    "\n",
    "**Motivation of using a power-test:** As previously stated, since we couldn't find a significant greater mean of systolic bp when comparing smokers to non smokers, and we already have our sample size, it is logical to see how refined our t-test actually was in finding even small differences. Therefore, we do this power-test.\n",
    "\n",
    "In checking power of that t-test i use a for loop which uses the basics of a t-test, creating some random datasets, doing t-test and viewing if the result came back as significant. However, there is a better method for this i found at https://docs.scipy.org/doc/scipy//reference/generated/scipy.stats.power.html . This does it faster since it doesn't need to loop 1000 times, but since i'm in the beginnig of my statistical journey, my method is easier to follow for a novice. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615976bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(smokers.describe())         #<-- To check if the correct n and std have been plugged in\n",
    "# print(non_smokers.describe())     #<-- To check if the correct n and std have been plugged in\n",
    "\n",
    "checking_power_of_that_t_test(213, 587, 13.2678 ,12.626038, 2, 0.05, 1000, \"greater\")\n",
    "checking_power_of_that_t_test(213, 587, 13.2678 ,12.626038, 3, 0.05, 1000, \"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b36ff",
   "metadata": {},
   "source": [
    "**Bonus-plot** \n",
    "\n",
    "**Notes:** Since having our print in our \"checking_power_of_that_t_test\" we get some ugly prints when running our plot, since our plot uses the function repeatedly.\n",
    "\n",
    "**Conclusion:** Here we actually calculate the difference that our t-test will find with a power of 80%, so we find that we will find a difference of 2,5 mmHg.\n",
    "\n",
    "**Source:** The one and only Joakim -> https://www.youtube.com/watch?v=tAW2Komgulk&t=4164s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7926ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_power_of_that_t_test(213, 587)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda44c44",
   "metadata": {},
   "source": [
    "**Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from src.regression_analysis import *\n",
    "from src.moduls import * \n",
    "from src.io_utils import *\n",
    "from src.viz import *\n",
    "from src.viz_part2 import *\n",
    "from src.PCA_and_PCR import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540e587f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mclean_data\u001b[49m(read_data(\u001b[33m\"\u001b[39m\u001b[33mdataset/health_study_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_data' is not defined"
     ]
    }
   ],
   "source": [
    "df = clean_data(read_data(\"dataset/health_study_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4133e4",
   "metadata": {},
   "source": [
    "**Linear regression**\n",
    "\n",
    "To see how our numerical variables height, weight, cholesterol and age correlate with systolic bp, we do a linear regression where we are interested in our slope (our linear coefficient which tells us how much our independent variable affects our dependent variable) and our r square value (which tells us how much of the variance in our dependent variable is explained by our independent variable).\n",
    "\n",
    "Below we see the results of our four linear regressions:\n",
    "1. Our r squared number is way too low, which can also be seen in the plot (that our dots are quite far away from our linear function). This indicates that height might not have too much to do with systolic bp according to our data.\n",
    "2. Same problem as previous.\n",
    "3. Now we have our r-squared value at 0.136 which tells us that cholesterol could actually have something to do with systolic bp, but our r square is still too low.\n",
    "4. Age and systolic bp get's an r-square value of 0.369, which tells us that age and systolic bp could in fact be correlated. And our slope is 0.54, meaning that for every year you age, your systolic bp generally goes up with 0.54 mmHg.\n",
    "\n",
    "Our next step is to look at our residual diagnostics, to check if our linear regression is satisfying the neccessary conditions of a linear regression.\n",
    "\n",
    "**Residual Diagnostics**\n",
    "- Based on the four plots provided for the regression model, the residual diagnostics indicate that the model meets our conditions required for our linear regression.\n",
    "\n",
    "1. Residuals vs Fitted Plot (Top Left)\n",
    "The residuals are randomly scattered around the horizontal line at zero.\n",
    "*Conclusion: This suggests that the linearity assumption is met, and there is no weird patterns visible.*\n",
    "\n",
    "2. Normal Q-Q Plot (Top Right)\n",
    "Most data points closely follow the straight red line.\n",
    "*Conclusion: This confirms that the residuals are approximately normally distributed, which is a crucial assumption for performing hypothesis tests and calculating confidence intervals.*\n",
    "\n",
    "3. Scale-Location Plot (Bottom Left)\n",
    "The points are spread evenly across the range of fitted values, showing no weird pattern.\n",
    "*Conclusion: This indicates homoscedasticity (that we have constant variance).*\n",
    "\n",
    "4. Histogram of Residuals (Bottom Right)\n",
    "The distribution is bell-shaped and symmetric around zero.\n",
    "*Conclusion: This confirms the finding from the Q-Q plot that the residuals are normally distributed.*\n",
    "\n",
    "**Source:** \n",
    "- https://www.youtube.com/watch?v=iMdtTCX2Q70\n",
    "- https://www.youtube.com/watch?v=sDrAoR17pNM\n",
    "- https://youtu.be/ZsJ-DbKpD3s?t=2660\n",
    "- https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html \n",
    "- https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html <-- För att se hur residuals sparas (resid)\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html\n",
    "- https://www.statology.org/standardized-residuals-python/\n",
    "- Claude and Gemini (helping with the residual diagnostics of Q-Q and scale-location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "use_reg = RegressionAnalysis(df)\n",
    "\n",
    "slope, intercept, r2, model = use_reg.linear_regression(\"height\", \"systolic_bp\")\n",
    "plotting_that_linear_regression(df, \"height\", \"systolic_bp\", slope, intercept)\n",
    "\n",
    "slope, intercept, r2, model = use_reg.linear_regression(\"weight\", \"systolic_bp\")\n",
    "plotting_that_linear_regression(df, \"weight\", \"systolic_bp\", slope, intercept)\n",
    "\n",
    "slope, intercept, r2, model = use_reg.linear_regression(\"cholesterol\", \"systolic_bp\")\n",
    "plotting_that_linear_regression(df, \"cholesterol\", \"systolic_bp\", slope, intercept)\n",
    "\n",
    "slope, intercept, r2, model = use_reg.linear_regression(\"age\", \"systolic_bp\")\n",
    "plotting_that_linear_regression(df, \"age\", \"systolic_bp\", slope, intercept)\n",
    "plot_residual_diagnostics(model)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e2f45",
   "metadata": {},
   "source": [
    "**Multiple linear regression**\n",
    "\n",
    "To see how our different variables correlate with systolic bp, we do a multiple regression, which isolates one variable at a time and check how that variables unique influence correlates with systolic bp, where we both get a coefficient (how much the isolated variable affects systolic bp) and a p value, which tells us if the correlation is actually significant. \n",
    "Before we do a multiple regression we first make our text-based categories \"smoker\" and \"sex\" info numerical values, so that we may try these variables as well.\n",
    "\n",
    "**Result**\n",
    "- R-squared: 0.407 which means that our multiple regression explains 40.7% of the variance in systolic bp.\n",
    "- Coefficients: Top 3 variables that have the greatest coefficients (effect) is **age**, **cholesterol** and if you have the **disease** or not.\n",
    "- p-value: Looking at the p value, only **age** and **weight** have a p value less than 0.05, which means that we cannot reject our null hypothesis for the variables height, cholesterol, smoker, disease and sex. And our null hypothesis is in this case that there is no correlation between the independent variable and the dependent variable.\n",
    "- Notes: In the model summary we got this note: *\"The condition number is large, 4.88e+03. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems.\"* \n",
    "Multicollinearity means that some of our independent variables might be affecting the other. For example, height and weight might affect one another and disturb our results in our multiple regression.\n",
    "\n",
    "\n",
    "**Source:**\n",
    "- https://www.datarobot.com/blog/multiple-regression-using-statsmodels/ <-- Regression-code\n",
    "- Claude for fixing a nice print format with β-symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ed1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(df[\"disease\"])\n",
    "\n",
    "df['sex_numerical'] = 0\n",
    "df.loc[df['sex'] == 'M', 'sex_numerical'] = 1\n",
    "\n",
    "df['smoker_numerical'] = 0\n",
    "df.loc[df['smoker'] == 'Yes', 'smoker_numerical'] = 1\n",
    "\n",
    "use_reg = RegressionAnalysis(df)\n",
    "use_reg.multiple_regression(\"systolic_bp\", [\"age\", \"weight\", \"height\", \"cholesterol\", \"smoker_numerical\", \"sex_numerical\", \"disease\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7d443",
   "metadata": {},
   "source": [
    "**VIF (Variance inflation factor)**\n",
    "\n",
    "To see how much each independent variable from our previous multiple regression is affected by the other independent variables, we do a VIF (Variance Inflation Factor) to detect multicollinarity. \n",
    "\n",
    "**Result**\n",
    "- Smoking, sex and however the person had the disease or not, had a relatively low VIF number, which tells us that those three variables are NOT or only in a small degree subjected by collinearity of the other independent variables. But neither of these variables had a p-value < 0.05 (they were all way higher), which tells us that we cannot reject our null hypothesis for these variables anyway.\n",
    "- Cholesterol, height, weight and age have an extremely high VIF number, which indicates that those variables can be predicted by the other independent variables and are therefore subjected by collinearity. \n",
    "\n",
    "**Source**\n",
    "- https://www.geeksforgeeks.org/python/detecting-multicollinearity-with-vif-python/ \n",
    "- https://www.statology.org/how-to-calculate-vif-in-python/ <--Det mesta av koden, med liten modifikation i ordningen\n",
    "- https://quantifyinghealth.com/vif-threshold/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reg = RegressionAnalysis(df)\n",
    "\n",
    "use_reg.calculate_vif([\"age\", \"weight\", \"height\", \"cholesterol\", \"smoker_numerical\", \"sex_numerical\", \"disease\"])\n",
    "\n",
    "use_reg.calculate_vif([\"age\", \"weight\", \"height\", \"cholesterol\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2692f",
   "metadata": {},
   "source": [
    "**PCA**\n",
    "\n",
    "We do a principal component analysis to see which variables we can cluster together to a set of new variables. We also get to see how much of the variance in our dataset is explained by our new PC-variables.\n",
    "\n",
    "**Result**\n",
    "- PC1 explains 39% of our datas variance! And the main contributors to PC1 is age and cholesterol. Therefore, we can internally name our PC1 \"Age related health-factors\"\n",
    "- PC2 explains 32.4% of our datas variance, and the main contributors here is weight and height. Therefore, we internally name our PC2 \"Physical size\"\n",
    "- Together PC1 and PC2 explains 71.3%, which is why we keep only these two PC-components.\n",
    "\n",
    "So now we have two new variables PC1 (age related health-factors) and PC2 (physical size), which we can do a PCR (Principal component regression) on with our dependent variable systolic bp.\n",
    "\n",
    "**Source**\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- Claude <-- The heavy lifting of this code\n",
    "- https://www.youtube.com/watch?v=rIKVoHp1HLM <--fit_transform\n",
    "- https://www.youtube.com/watch?v=HWkM1kH52d8 <-- eigenvalue and eigenvector\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fe131",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vars = ['age', 'height', 'weight', 'cholesterol']\n",
    "\n",
    "use_pca_or_pcr = PCA_and_PCR(pca_vars, df)\n",
    "\n",
    "results = use_pca_or_pcr.pca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d0f35",
   "metadata": {},
   "source": [
    "**PCR (Principal component regression)**\n",
    "\n",
    "Doing a new regression with our PC variables eliminates the problem of multicollinearity. \n",
    "\n",
    "**Result**\n",
    "- Explained variance by PC1 and PC2 of our total data is 71.3%, which means that PC1 and PC2 together explains the vast majority of the variance in our independent variables variables age, height, weight, cholesterol.\n",
    "- R-squared: 32.34%, which means that about 32% of our variance in systolic bp is explained by PC1 and PC2. Altought R2 is lower in our multiple regression (about 40%) we know that PC1 and PC2 doesn't have problems with multicollinearity (VIF stable around 1). This is a really good tradeoff!\n",
    "- Coefficients: PC1 (\"Age related health-factors\") increases systolic bloodpressure by 5.7 mmHg for every increase of 1 PC1 unit. PC2 on the other hand decreases systolic bloodpressure by 1.26 mmHg for every 1 unit increase of PC2. This tells us that age related health-factors has a positive correlation with a relatively strong effect on systolic bloodpressure (which was expected), while, interestingly, PC2 (\"Physical size\") has a negative correlation with systolic bloodpressure, meaning that the larger you are as a person, the lower your systolic bloodpressure will be. \n",
    "\n",
    "**Analysis** \n",
    "- Let's think about what constitutes PC1: age, weight and cholesterol. But what constitutes PC2? Well, there we have height and weight (again), but looking at PC2 we also see a negatively association with age. So even though it was expected that age related health-factors has a positive correlation with systolic bloodpressure, now imagine that PC2 is about a person being large (which we would generally think would lead to a higher systolic bloodpressure), but PC2 is physical size WHEN age has been removed from the equation. So maybe the people within PC2 are physically fit (which equals larger bodymass), and this leads to a higher systolic bloodpressure. Food for thought!\n",
    "\n",
    "**Source**\n",
    "- Same as PCA combined with linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pca_or_pcr.pcr(\"systolic_bp\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f46dd",
   "metadata": {},
   "source": [
    "**Summary of part 2**\n",
    "\n",
    "- Linear regression: We first did 4 different linear regressions where we tried the independent variables: height, weight, cholesterol and age to see how they correlated with our dependent variable systolic bloodpressure. The results were that only age seemed to explain the variance of systolic bp. Then we did a residual diagnostics to see if our linear regression met the neccessary conditions - which it did! \n",
    "- Multiple regression: Then we did a multiple regression to see how our independent variables explained the variance in our dependent variable systolioc bp in a statistical significant way. Our results were that only age and weight had a p-value below 0.05, however, we got a note that there was probably high multicollinearity between the variables.\n",
    "- VIF: To see which variables were subjected of multicollinearity, we did a VIF (variance inflation factor). The results were that only smoking and sex had a relatively low VIF, but remember from our multiple regression that smoking and sex did not give a result of statistical significance. So we redid our VIF without sex, disease and smoking, to see if we could lower our VIF for the other variables. The results were that age, weight, height and cholesterol still had way too high VIF results.\n",
    "- PCA: So next, to eliminate multicollinearity, we did a PCA (principal component analysis) and came up with two new variables: PC1 (\"Age related health-factors\") consisting of age, weight and cholesterol, and PC2 (\"Physical size\") consisting of weight and height. Together PC1+PC2 explained 71.3% of our variance in our data, which makes it suitable to exchange all our independent variables for PC1 and PC2, knowing we have our multicollinearity regulated.\n",
    "- PCR: And finally, we did a PCR (principal component regression) to see how our PC1 and PC2 explained the variance in systolic bp. Our results gave us that PC1 and PC2 explained 32% of the variance in systolic bloodpressure, while PC1 was positively correlated with systolic bloodpressure (with a strong effect), and PC2 was negatively correlated with systolic bloodpressure (with a mild/low effect). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
